\documentclass{book}
\usepackage[utf8x]{inputenc}
\usepackage{textgreek}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathpartir}
\usepackage{lineno,xcolor}
% TODO notes
\newcommand{\TODO}[1]{\textcolor{red}{[TODO: #1]}}
% Running line numbers:
\linenumbers
\setlength\linenumbersep{5pt}
\renewcommand\linenumberfont{\normalfont\tiny\sffamily\color{red}}
\pagewiselinenumbers
\newcommand{\tr}[2]{\| #2 \|_{#1}}
\newcommand{\trcon}[1]{| #1 |}
\newcommand{\conn}{\mathsf{conn}}
\DeclareMathOperator{\fix}{\mathsf{fix}}
\newcommand{\red}{\to}
\DeclareMathOperator{\Later}{\vartriangleright}
\DeclareMathOperator{\dfix}{\mathsf{dfix}}
\DeclareMathOperator{\prev}{\mathsf{prev}}
\DeclareMathOperator{\next}{\ensuremath{\mathsf{next}}}
\DeclareMathOperator{\Cube}{\square}
\newcommand{\sd}{.\,}
\begin{document}

\chapter{Introduction}

Martin-Löf, logic based on computation, proposition-as-types,
possibility programming language, because canonicity tells us we get results
if we run computations.

  Successes in verification of mathematics and CS, see Anders thesis for Gonthier stuff, odd-order groups, CompCert.

  dependent types also as expressive type system for a functional
  programming language, types guide development, type inference can in
  specific cases generalize to program synthesis, while in general
  invariants encoded in types constrain the possible programs towards
  the correct one (cite Conor ``gravity well'' thing).
  %% you can program your types too!

  In this style where programs and proofs (of partial correctness) are
  intertwined, the latter can become a burden that litters the
  programs, which is why a variety of techniques has been developed to
  mitigate the needs for proofs, like careful definitions mindful of judgmental equalities, small and
  large scale reflection, other forms of tactics (cite pivotal?).

  Invariants can also be left out of programs' types entirely, but
  even then we would still be left with the burden of writing a
  program that the theory can recognize as total, which is something
  that mainstream languages do not request. The burden is significant
  enough that implementations like Agda and Idris provide pragmas to
  circumvent it and instead accept the programmers judgment and/or
  mark the definition as untrusted. (does Coq have something like this?).

  A core calculus would typically ensure totality by only providing
  (co)induction combinators, which have the benefit of being easy to
  model, and fit well with categorical semantics as universal
  properties.  They correspond to primitive (co)recursion or folds,
  which, as witnessed by the development of powerful generalizations
  (cite ``Unifying Structured Recursion Schemes''), are not easy to
  use directly.The state of the art in proof assistants based on type
  theory is instead to allow pattern matching and direct recursion,
  and deploy more or less sophisticated coverage and termination
  checks, which none the less are fairly limiting, especially because
  they do not allow the programmer to provide their own reasoning to
  convince the checker.


\section{This Thesis}

This thesis is a collection of six papers divided into three parts.
The first deals with the induction principle of n-truncations in HoTT.
The second with guarded types and sized types as type-based criteria
for termination and productivity. The third with decidability of
conversion of type theory.

\subsection{First part}
The issues with writing programs we discussed so far
\TODO{Recapitulate the relevant issues}
also apply to
what might seem closer to formalizations of mathematics, as soon as the
latter involves non-trivial constructions.
Homotopy Type Theory (cite book) is a field that connects homotopy theory and type theory.
The connection centers around the identity type, whose elements can be
thought of as paths connecting the two values being equated. Types are
then interpreted as topological spaces up to homotopy.

It then becomes natural to classify types according to the complexity
of their topology, we call a type \emph{contractible} if it is
equivalent to \TODO{Should ``equivalent'' be made more precise?  In
  the context, would it mean ``continuously deformable''?  Maybe the
  topological interpretation could be elaborated a bit more...}
the unit type, \TODO{Parallel this (``unit type'') with the
  topological equivalent (e.g. ``one-point space''!?)}
then we say that a type has homotopy level $n$ (or is an
$n$-type) if its ($n{+}2$)-iterated identity type is contractible.
\TODO{Also spell out the $-2$-case, the $0$-iterated identity type.}
As an example,
$0$-types behave much like discrete spaces, since all paths are
trivial, \TODO{Elaborate ``trivial'' like, ``i.e., continuously
  deformable into the degenerate, constant/stationary path''}
and so are a suitable representation of sets \TODO{cite Bas' set theory stuff?},
while ($-1$)-types are regarded as mere propositions, i.e.,
proof-irrelevant. \TODO{...meaning that all elements of a $-1$-type are
  connected, and it only matters whether there is some element or none
  at all}
It happens that some constructions, like pushouts, do not naturally
preserve the homotopy level, e.g., build sets out of sets.  Or maybe we
would like to turn constructive existence into a mere proposition.  For
these and other reasons it can be useful to truncate a type to a
desired homotopy level.
Given an arbitrary type $A$, its $n$-truncation $\tr{n}{A}$ is the least
$n$-type with a map $\trcon{{-}} : A \to \tr{n}{A}$.  Equivalently, its standard
induction principle says that a map $\tr{n}{A} \to B$ can be built
from $A \to B$ as long as $B$ is also an $n$-type.
In the paper ``Functions out of Higher Truncations'' we relax this to
$B$ being an ($n{+}1$)-type as long as the function is constant on all
($n{+}1$) loop spaces, \TODO{give an intuition for $m$-loop space}
which we call $n$-constant.
The paper gives two proofs, one based on reformulation of $\tr{n}{A}$
as an ($n{+}1$)-truncated HIT \TODO{spell out and reference HIT}
with extra constructors, the other on the
$n$-connected components of the type $A$:
\[
\conn_n(x) := \Sigma (a : A)\sd \trcon{a} = x
\]
\TODO{$n$ is not used on the rhs of this definition!?}
by contractibility of singletons we have the following equivalence
\[
A \simeq \Sigma (x : \tr{n}{A})\sd \conn_n(x)
\]
from that we can show that an $n$-constant map $A \to B$ is a family of $n$-constant
maps $f_x : \conn_n(x) \to B$. Each of the $n$-constant $f_x$ is actually
determined by a single point in $B$, which is not surprising since
collectively they are supposed to correspond to a map from
$\tr{n}{A}$. This motivates us to prove the following family of equivalences,
\[
\Pi\,x : \tr{n}{A}\sd (B \simeq \Sigma (f_x : \conn_n(x) \to B)\sd\textsf{$n$-constant}(f_x))
\]
\TODO{Is the use of $\Pi$ necessary here?  Couldn't you say that $B
  \simeq ...$ for any $x : \tr n A$?  Btw.\ this is quite surprising
  and would deserve more explanation.  The rest of $B$ seems to be
  packed into the ``$n$-constant'' statement.}
we do so by induction on $n$ and by the fact that being an equivalence is a
($-1$)-type which allows induction on $x$. \TODO{How this ``induction
  on $x$'' looks like would be interesting to know.}
We can then collect this
family of equivalences into an equivalence of families, which
concludes the result.
\TODO{I see that probably here the $\Pi$ plays out.}

This extended elimination principle has been used in ``Constructions
with Non-Recursive Higher Inductive Types'' \TODO{cite} to give a
definition of propositional truncation without making use of recursive
higher inductive types. The same paper provides alternative
elimination principles for $n$-truncations, but while they allow the
codomain to be an ($n{+}k$)-type for an arbitrary $k$, they impose
stronger than expected constraints on the map $A \to B$, so there is
still further work to be done in this direction.



\subsection{Second Part}

motivate type-based totality checking (quite broader origins actually, then applied to type theory!)

  \begin{itemize}
  \item show general problems, copy some examples from papers.
  \item idea: approximations of mu/nu up to an ordinal. keep track of stages in types
  \item (well-founded) induction on stages as a typing rule for recursive definitions/semantic model
    - examples
  \item overlooked in sized-types: some approximation chains never truly
  stabilize but are useful none-the-less: guarded types to abstract
  over step-indexing models. still terminating as types and fixpoints of sized functors.
  \item Guarded types do this using Nakano's modality instead of explicit stages. (guarded recursive types: recursion in types only under later)
    \begin{itemize}
    \item give the Fref example.
    \item also tying the knot example from Atkey?
    \end{itemize}

  \item Funnily enough such fixed-points can be back-ported to
    sized-types with a sufficiently strong language (e.g. higher-rank
    size quantification):
    \begin{itemize}
    \item example: fix with SizeLt. Type of fix is quite different from
    $(\mathsf{Later} A \to  A) \to A$ because we do not assume antitonicity.

    \item termination problems if we are so liberal with sizes: ..., Maybe Size< should be banned as a type and only SizeLt allowed.
    \item really considering sizes as ordinals your programs can do
         induction over gives a more accurate view of their power than sizes as just
         annotations to help with checking normalization of (co)recursive definitions over (co)data.
    \end{itemize}
    \end{itemize}
    %% Not So Sure About integrating this.
    %% - $\inf$ question: what is $fix (\ i X -> Later i X -> X) \inf$
    %% supposed to be? Do we really need $\inf$, or should we rather have
    %% unsized-fixpoints and subtyping/overloading? (c.f. Amadio 98 but
    %% also Abel13 typing rules for constructors)
    %%  - Agda makes it complicated by $\inf < \inf$ and $\inf + 1 = \inf$ sometimes.
    %%  - and generally the structural termination checker gets a bit confused with liberal sizes and uses of $\inf$
    %%      - sizes can really make a seemingly-inductive type into more of a
    %%      coinductive one because of unique fixpoints, and so the structural
    %%      termination checker gets confused.
    %%      %% I Like This Part.



%% -- state-of-the art and contributions i guess?

\subsubsection{Guarded Types}

In order to integrate the $\Later$ modality into intensional type
theory it is necessary to develop its operational semantics, even in
the presence of free variables, so that we can formulate a useful but
decidable judgmental equality. The following two papers confront the
challenge to unrestricted computation that the guarded fixed point
poses.

  \paragraph{``A Formalized Proof of Strong Normalization for Guarded Recursive Types''}
  studies a simply typed lambda calculus with products extended with guarded
  recursive types and the applicative functor structure of the $\Later$
  modality:
  \[
  \begin{array}{l}
  \next : A \to \Later A \\
  \star : \Later (A \to B) \to \Later A \to \Later B
  \end{array}
  \]
  a guarded fixed point combinator $\fix : (\Later A \to A) \to
  A$ can be derived using the recursive type $T = \Later T \to A$.
  %% Why strong normalization. ``strong'' maybe overkill, but in
  %% general we need more than head normalization.
  Taking reduction to be the congruence closure of the usual $\beta$ rules
  for application and projections plus the following law $(\next t)
  \star (\next u) \mapsto \next (t\,u)$, would lead to an infinite
  chain $\fix f \red f\,(\next (\fix f)) \red f\,(\next (f\,(\next (\fix f)))) \red
  \ldots$, invalidating strong normalization.

  We instead lift the concept of finite approximations directly from
  the model and define a reduction relation indexed by a natural
  number $n$, which specifies under how many nested uses of $\next$ we
  are still allowed to reduce.

  This strategy takes inspiration from the denotational semantics: if
  a term is supposed to represent a natural transformation in the
  topos of trees, $\mathsf{Set}^{\omega^\mathsf{op}}$, then we are
  entitled to apply it to a natural number and observe its behaviour
  at that stage, but the semantics of $\next$ ignore its argument at
  stage zero, and it is this base case that bootstraps the recursion
  that justifies $\fix$, so it seems natural for reduction to also
  stop there.

  The proof itself goes by showing that saturated sets (cite someone),
  indexed by the stage as a natural number, form a model of the
  calculus. By saturated set we mean a predicate over terms which sits
  between the strongly neutral and the strongly normalizing terms, and
  moreover supports renaming and is closed under strong head
  expansion. Indexing by the stage is necessary because our notions
  of neutral, normal and expansion inherit the stage from the
  reduction strategy indicated above.
  Eventually we prove that the predicates corresponding to the various
  type formers are antitone with respect to the stage, so it might be
  worthwhile to reformulate the model as a refinement of the gluing of
  the category of presheaves over typing contexts and stages, along the functor that
  builds the presheaf of terms out of a type (cite Hofmann, Thorsten, ..?).

  The fact that a term might have a different normal form for each
  stage introduces some problems for using this as a strategy to
  decide equality though, because later stages will generally
  recognize more terms as reducing to the same normal form.
  For base types and with a restricted context the stage does not
  matter, as shown in (cite Rasmus and Patrick) but in general we do
  not provide a bound on which stages to try when comparing two terms.
  An option would be to design a type theory where the staging is expected
  to influence the judgmental equality: one possibility is to consider
  the stage as a fuel we earn by walking down our terms during
  typechecking and then get to spend in the conversion rule when
  comparing equality of types.
  We would have a typing judgment of the form $\Gamma \vdash_n t : A$, where the stage would increase under a $\next$:
  \[
  \inferrule*{\Gamma \vdash_{n+1} t : A}
             {\Gamma \vdash_n \next t : \Later A}
  \]
  and then used by conversion:
  \begin{mathpar}
  \inferrule*{\Gamma \vdash_n t = u : A}
             {\Gamma \vdash_{n+1} \next t = \next u : \Later A}
  \and
  \inferrule*{ }
             {\Gamma \vdash_0 \next t = \next t : \Later A}

  \end{mathpar}
  top-level definitions would then be checked at stage zero. The
  usefulness of such a system and whether it enjoys the usual expected
  meta-theoretic properties could be interesting future work.

  Finally, since the strong normalization proof is fully formalized in
  Agda it has been stripped of the later modality components and taken
  as the prototype implementation for the POPLMark Reloaded challenge (cite it),
  which aims to compare different approaches to syntax with binding in
  proof assistants by how they apply to a fairly involved but
  well-understood proof like strong normalization for the simply typed
  lambda calculus.

  %% Notes
  %% \begin{itemize}
  %%   \item Takes the ``finite approximations'' model at face value and
  %%     blocks reduction when deeper than n ``next''.
  %%   \item Recursive types introduced by a coinductive type of types, term-level fix then definable by exploiting built-in type-level recursion.
  %%   \item proof with Saturated sets and inductive SN characterization.

  %%   \item the stratification makes it awkward to use to decide, and which
  %%   equality: at which n should things be compared? larger n's admit
  %%   more things as equal, but what's a canonical choice?
  %%   Maybe use the typecheking context/keeping track of how many next
  %%   we have gone under? But not the dynamic context, that can be
  %%   fooled into growing. Is this stable under substitutions?
  %%   \item stripped of the guarded stuff, taken as prototype impl. for poplmark reloaded.
  %% \end{itemize}

  \paragraph{Guarded Cubical Type Theory: Path Equality for Guarded Recursion}
  presents Guarded Cubical Type Theory (GCTT) as an intensional
  version of the extensional Guarded Dependent Type Theory (GDTT)
  (cite GDTT).

  As the name suggests GDTT is a dependent type theory, including
  $\Pi$ and $\Sigma$ types, a natural number type, an equality type, and a hierarchy of
  universes, extended with the $\Later$ modality and clock
  quantification. The main innovation is the introduction of delayed
  substitutions in place of the $\star$ operator: they appear as extra
  arguments of $\next$ and $\Later$ and allow the formation of types
  like $ A : \mathbb{N} \to \mathsf{U}, x : \Later \mathbb{N} \vdash
  \Later [n \leftarrow x]\sd A\,n$.
  GDTT furthermore makes $\fix$ a primitive, since in the presence of
  a universe it can be used to derive guarded recursion in types,
  instead of the other way around. The theory is motivated by a few
  examples of non-trivial corecursion patterns and proofs of equality
  of guarded streams by bisimulation.

  %% Examples of applicability of GDTT?
  Judgmental equality is undecidable, not just because of the
  reflection rule, but also due to the rule for unfolding $\fix$,
  $\Gamma \vdash \fix f = f\,(\next (\fix f))$, which introduces the
  same normalization problems as discussed above. Just removing the
  reflection rule and turning the unfolding of $\fix$ into a
  propositional equality would likely recover decidability but destroy
  canonicity, as now there would be proofs of equality that are not
  given by reflexivity and the eliminator for the equality type would
  be stuck on them.

  This is a problem that arises whenever we start from an intensional
  type theory where propositional equality is seen as an inductive
  type with reflexivity as the only constructor, and correspondingly a
  single $\beta$ rule for the eliminator: canonicity dictates that in
  a closed context propositional equality and judgmental equality must
  coincide, so decidability of the latter also severely limits the
  former. Breaking this coincidence was the main feature of Observational Type Theory (cite OTT) which got around it by
  internalizing the setoid model of type theory and defining
  propositional equality on a type by type basis, and so achieving
  support for function and proposition extensionality\footnote{And expected to also support quotient types and bisimilarity for coinductive types}.

  Cubical Type Theory (CTT) (cite CTT) takes the intuitions of Homotopy
  Type Theory about the identity type a step further, and uses paths
  as the representation of equality proofs themselves, as in maps from
  an abstract interval type. Then reflexivity is not the only
  canonical element of the equality type, function extensionality just
  follows from abstraction for paths and function types, and
  univalence can be proven with the support of a special
  $\mathsf{Glue}$ type that allows to build paths in the universe from
  equivalences. This does not come for free, as now the eliminator for
  paths has to deal with more canonical forms, and its
  behaviour is defined according to the type we are eliminating into.

  We obtain GCTT by extending CTT with the $\Later$ modality and $\next$ as in GDTT and adding a path $\mathsf{dfix}$
  \begin{mathpar}
    \inferrule{\Gamma \vdash f : \Later A \to A \quad \Gamma \vdash r : \mathbb{I}}
              {\Gamma \vdash \dfix f\,r : \Later A}
    \and
    \inferrule{}
              {\Gamma \vdash \dfix f\,1 = \next\,(f\,(\dfix f\,0)) : \Later A}
  \end{mathpar}
  so that we can define $\fix f := f\,(\dfix f\,0)$ and prove it
  equal to its one step unfolding by $<i> f\,(\dfix f i)$.
  Choosing $\dfix$ as the primitive rather than $\fix$ avoids the
  introduction of new canonical forms at an arbitrary type $A$, and
  confines them to $\Later A$ instead.
  Since GCTT does not have clock quantification there is no real
  eliminator for the $\Later$ modality so there's no issue with piling
  up more term constructors for it, however the prototype
  implementation (cite github) has some limited support for clock
  quantification and we were able to make $\dfix$ step forward in a
  controlled fashion when implementing the computational behaviour of
  $\prev$\footnote{here we give only a simplified type}:
  \begin{mathpar}
    \inferrule{\Gamma \vdash_{\Delta, \kappa} t : \Later A}
              {\Gamma \vdash_\Delta \prev \kappa\sd t : \forall \kappa\sd A}
              \and
              \inferrule{}{\prev \kappa\sd \next t = \Lambda \kappa\sd t}
  \end{mathpar}
  In fact we can simply reduce as if $\dfix f\,r = \next\,(f\,(\dfix f\,r))$ and let $\prev$ strip away the $\next$.
  \[
    (\prev\,\kappa\sd \dfix f\,r) = \Lambda \kappa\sd f\,(\dfix f\,r)
  \]
  %% TODO claim we invented labels, and Patrick made a type system for them.

  The paper provides a a denotational model in presheaves over $\Cube
  \times \omega$ which proves the consistency of our theory.
  The category of cubes $\Cube$ is the one used for the denotational
  semantics of CTT, so our model can be thought of as staged cubical sets.

  To be fair if the aim is just to extend CTT with coinductive types
  whose equality is bisimulation we do not need to introduce the
  $\Later$ modality. As I discuss in (cite cocubical
  note)\footnote{although only for the case of streams} , cubical sets
  already supports coinductive types in the sense of strict final
  coalgebras\footnote{being a presheaf category they can be built as
    limits, level-wise}, and they can be shown to be fibrant,
  i.e. support the eliminator for the equality type.  Then we just
  need to have a theory that allows to define paths corecursively, and
  for that copatterns seem to be a very good fit, as shown by this
  example in ``Cubical Agda''\footnote{the option --cubical available from the master branch of the Agda proof assistant at the time of writing}.
\begin{verbatim}
--   TODO use agda --latex
record Stream (A : Set) : Set where
  coinductive
  constructor _,_
  field
    head : A
    tail : Stream A

open Stream

map : ∀ {A B} → (A → B) → Stream A → Stream B
head (map f xs) = f (head xs)
tail (map f xs) = map f (tail xs)

map-id : ∀ {A} {xs : Stream A} → map (λ x → x) xs ≡ xs
head (map-id {xs = xs} i) = head xs
tail (map-id {xs = xs} i) = map-id {xs = tail xs} i
\end{verbatim}
  We have that \verb|map-id {xs = xs}| is a path between streams, so it
  takes an interval variable \verb|i| as argument, and then it defines a
  stream by giving its \verb|head| and \verb|tail| in such a way that they match
  the \verb|head| and \verb|tail| of the streams we are proving equal.  For the
  \verb|tail| case we proceed by corecursion, as we would in case we were
  defining a simple function, rather than a path.  This use of
  corecursion is still justified in the denotational semantics because
  we model streams by taking a final coalgebra across arbitrary
  cubical sets, so the universal property gives us a map even from a coalgebra that
  has the interval type as a component.

  Of course the usual limitations of productivity checking apply, so
  integration with type based approaches is still important.

  %% \begin{itemize}
  %%   \item GDTT, extensional dependent type theory with guarded types:
  %%   conversion is undecidable, because of reflection rule but also
  %%   because of unfolding-fix rule.

  %%   \item Removing reflection rule and making unfold-fix just a
  %%   prop. equality would likely recover decidability but kill
  %%   canonicity: we can prove closed streams prop. equal which won't be
  %%   definitionally equal, so some closed proofs won't reduce to
  %%   ``refl'' and J on them will get stuck.

  %%   \item Need a more forgiving identity type, one where more proofs are
  %%   canonical: enter cubical type theory.
  %%   CTT takes the intuitions of Homotopy Type Theory a step
  %%    further, and uses paths as the representation of equality proofs themselves,
  %%    as in maps from an (abstract) interval type.
  %%    Then function extensionality just follows, and even univalence by a special Glue type.
  %%    The price to pay is more complexity in the eliminator for these paths
  %%    , which in fact has to be given computational meaning on a type by type basis.

  %%  \item GCTT then integrates the later modality and fix into CTT, having a path for fix-unfolding.
  %%    The technical trick is dfix and the path at dfix, not to mess with canonical terms at other types.
  %%    Not having any proper eliminations for later yet, we do not need to worry about what canonical terms do.
  %%    The prototype typechecker handles clock quantification and in that case we actually get dfix/comp/next unstuck.

  %%    We provide a denotational model in $Cubes \times \omega$
  %%    presheaves, which proves the consistency of our theory. The construction is presented from an internal
  %%    perspective as much as possible (cite Pitts,Bas,.. paper on how this internal style got expanded on).
  %% \end{itemize}

\subsubsection{Sized Types}
\begin{itemize}
    \item too many zeros problem / not able to actually prove isomorphisms.
    \item sizes make a difference when indexing types but shouldn't make a difference in values.
    \item need a way to indicate when size quantification should behave in a uniform way
      \begin{itemize}
      \item standard irrelevance like Pfenning does not work, because codomain relevant in sizes
      \item intersection types as in ICC* or something do not play well with eta.
      \end{itemize}

    \item if we look at the semantics of (co)inductive types as (co)limits
    of chains, then the condition we would like to impose is
    naturality. But internalizing naturality directly would require us
    to keep track of the variance of size variables in types and into
    the realm of directed type theory, which is not well understood
    yet.

    \item However the semantics of lambda calculus give us relational
    parametricity, a concept which also tries to characterize the
    well-behavedness or invariance of maps for some of their
    arguments, and indeed also specializes to naturality in many cases.

    \item Parametricity has been studied before in the context of MLTT,
    both in the sense of categorical models (cite Bob, and new Ghani
    stuff?) satisfying parametricity and as extensions of type theory
    internalizing parametricity principles (cite Guilhem).

    \item those works study how Pi itself enjoys interesting parametricity
    properties, in case the domain has an interesting notion of
    relation. However they are confronted with limitations with
    regards to trying to scale the identity extension lemma to
    universes, especially a hierarchy of them.

    The identity extension lemma, used to derive naturality from
    parametricity, would require that the relational interpretation of
    a closed type is just equality, but if this applied to our
    interpretation of the universe we wouldn't get any interesting
    theorem about polymorphic functions.

    Another way to look at this is that we would expect parametricity
    to imply that a function of type $f : (A : U) \to F A \to G A$
    would be a nat. trans. when F and G are functors.  But if we have
    U : $U_1$ and take F A = Unit, G A = U, we could write
    $f = \lambda (A : U)\, (\_ : F A)\sd U$ which does not fit into a naturality square.
    It seems that polymorphism-as-pi only guarantees naturality when
    the codomain cannot allow the information we are polymorphic over
    to leak.


\paragraph{Parametric Quantifiers for dependent type theory}
    The solution of ``ParamDTT'' is to introduce new quantifiers which
    restrict the use of the quantified variable through a system of
    modalities, thus preventing such leaks even when the type of the
    value we are currently building would be large/complex enough to
    allow it.

    With those and a type ``Size'' which supports parametric-and-non
    well-founded induction we are able to internally build indexed
    initial and final coalgebras for functors that satisfy an
    internal notion of (co)continuity (similar to the condition in
    Rasmus paper), which includes (finitary-branching) polynomial
    functors.

    The drawback is that our version of identity extension is
    introduced as a propositional axiom, and future work is needed to
    recover canonicity. The theory already uses concepts from
    CoquandBernardyMoulin and Cubical Type Theory to discuss values
    parametrically related, so we expect we can also take as model the
    composition operation of CTT to give computational meaning to this
    equality axiom.

    We give a denotational semantics in a presheaf model,
    $\hat{\mathsf{BCCube}}$ (todo: check name), of cubes generated by two distinct interval
    objects, with a map between them, which we use as domains of
    respectively paths and bridges, the former represent equalities,
    and the latter parametric relatedness.

    We also provide a prototype implementation as a fork of Agda, by
    adapting the present support for --cubical and an irrelevance
    modality. As a side-product, the effort spent adapting the support
    for modalities to the ones in ParamDTT then meant the
    implementation was easily adaptable to other schemes of modality,
    which then to a prototype implementation of crisp type theory
    (cite Pitts et. al paper).
    \end{itemize}

\paragraph{Normalization by evaluation for sized dependent types}
    \item In NbE for Sized Types we also explore irrelevant size
    quantification in a dependently typed language, but here
    irrelevance affects judgmental equality, and our technical
    contribution is an NbE algorithm for conversion checking.

    The main hurdle is how to reconcile typed judgmental equality with
    irrelevance, since given $f : \forall i : \mathsf{Size}. T i$ we would like
    $f~i$ and $f~j$ to be equal, but they would naturally live in
    different types $T~i$ and $T~j$.
    \footnote{In ParamDTT, irrelevance only gives prop. equalities,
      and we have a notion of heterogeneous equality over two
      parametrically related types}
    \begin{itemize}
    \item GDTT similarly has clock irrelevance, given $t : \forall
    \kappa\sd T$, $T \# \kappa$ then $t~\kappa = t~\kappa' : T$, the
    side condition makes the conclusion type correct
    \footnote{it is also required for soundness, but that's because
      clock quantification behaves more like an actual product than
      our $\forall$}
    , but it's not clear how to integrate the rule into a conversion
    algorithm as for example it can be used to prove
    $f : \forall \kappa\sd Bool \times T \kappa \vdash fst (f~\kappa) = fst (f~\kappa') : Bool$
    but only by reconstructing the term $t = \lambda \kappa\sd fst
    (f~\kappa)$, because the rule cannot be applied to $f$ directly
    due to its type.
    Maybe some form of anti-unification could be deployed but ``Clocks
    are ticking, no more delays'' plans on turning this into a
    propositional equality instead.

    \item Our paper takes a different approach and instead of a separate
    size irrelevance rule we make the typing and congruence rules for
    application of $\forall i. T$ to be irrelevant in the size, like so:

      TODO get rules from the paper

   The intuition is that we are approximating curry-style equality for
   church-style terms: the sizes in the application are there as
   annotations for a typechecking algorithm, but they are actually
   ignored by the type theory itself.

   Concretely, the reason we can be so liberal with sizes is that
   in our type theory they never affect the overall ``shape'' of a
   type, as size expressions only appear in size applications and as
   indexes of datatypes, and there's no size-case. That means that the
   same type directed eta equality rules will apply to both $t = u :
   T[i:=a]$ and $t = u : T[i:=b]$.

   \item We prove conversion decidable by presenting a Normalization by
   Evaluation algorithm its proofs of soundness and completeness.
   The nbe includes the usual reflection and reification steps, which
   take a type to orchestrate expansion to eta-long normal forms.
   The notion of type shapes comes back in the completeness proof, as
   the types provided to reflection and reification end up diverging
   from the type we want to compare the terms at.
   But all is well as long as the formers are the same approximate
   shape as the latter (defined formally in the paper), because they
   will trigger the same eta-expansions.
   We just have to generalize the corresponding theorems to get the
   induction through.

 \item limitations:
     \begin{itemize}
     \item still wish we could have an incremental whnf and
     head-comparison conversion checking. the algorithm would have to
     figure out a suitable size ``b'' to use in the type in the
     congruence rule, probably by some kind of unification algorithm.

     \item ignoring sizes in typing is not necessarily how you want to do
     it either, using the semantics from the parametric quantifiers
     paper as a guide we want to develop a theory with an
     heterogeneous judgmental equality.
     The judgment would be a decidable approximation of
     paths-over-a-bridge.
     \end{itemize}
    \end{itemize}

\subsection{Third Part}

\subsubsection{Decidability of Conversion}

The previous part of this thesis proposes extensions of type theory
aimed at increasing its expressivity. Such extensions are shown
consistent through various models but we also want them to be
practically implementable.

Proof assistants like Coq and Agda (cite?) insist on decidable
judgmental equality and normal forms for terms. This simplifies the
implementation by being able to omit the storage not only of typing
derivations but also type annotations in terms. That is because
redexes can sometimes be typed by fairly different derivations if we
don't have the necessary annotations, but normal forms do not have
this problem because we can deploy a bidirectional typechecking
algorithm and infer some of those annotations back as needed. Another
advandage of normalization is that one can rely on it for techniques
like small scale reflection, which make use of computation at the
level of types to automatically simplify goals rather than having to
produce a proof term that encodes each computation step.

When extending type theory then one would like to prove that such nice
properties are preserved, and ultimately that a reasonable
typechecking algorithm can be deployed. A big part is proving that
there is an algorithm for deciding whether two terms are equal at a
given type, i.e. conversion. It's not a straightforward task, because
the behaviour of types is complicated by universes allowing arbitrary
reductions in them, and the equality of terms is affected by types
through the eta rule for dependent functions.

If one only wants to show decidability, one option is to prove a
normalization theorem using, for example, a normalization by
evaluation technique as we did above. The decision procedure would
then be to fully normalize each term, then compare the normal forms.
However that is not how conversion checking is implemented in
e.g. Agda or MiniTT (cite), where the algorithm is instead to reduce
to weak head normal form, then compare the heads, and then recurse in
subterms. If normalization happens lazily then the two algorithms can
actually have very similar executions, but in such a setup it might be
hard to tell if we are normalizing more than necessary. Also a
practical typechecker probably wants to insert short cuts like
checking for syntactic equality to skip recursion on subterms when
possible. Side-stepping normalization is especially useful in the
presence of unification variables, the incremental comparison
algorithm can be easily extended to the case where one or both of the
terms is an application of an unification variable and attempt to
solve it without further reduction.
%% TODO say something about constraint solving and propagating type information?


\paragraph{Decidability of Type Theory in Type Theory} for the reasons above,
takes the option of formalizing the correctness of a conversion
checking algorithm based on typed weak head normalization and
type-directed comparison. Specifically we define algorithmic equality
as an inductively defined relation, which then we show decidable and
equivalent to judgmental equality.\footnote{Regarding the relationship with
normalization, from a proof that a term is algorithmically equal to
itself we can extract its normal form and a
proof that it is judgmetally to the term.}

Our proof goes by a kripke logical relation with contexts as worlds
and renamings as morphisms between worlds. The relation is over open
terms, hence the kripke structure, and based on reduction, in the
sense that membership is determined by the weak head normal form
(whnf) of a term, because of this we call it reducibility. For types
the relation is defined inductively and just recurses in the subterms
of its whnf, for terms in a type it is defined by recursion on the
proof that the type is reducible, and it checks that the observations
that are possible for the whnf of the term are also reducible. By the
same recursion we also define notions of reducible equality for types
and terms. The fundamental theorem is proven as usual by quantifying
over a reducible substitution\footnote{however we package this
  quantification into its own $\Vdash^v$ relation}, and by showing
that the identity substitution is reducible. It is only after this
proof that a lot of intuitions about the typing and equality judgments
are validated: injectivity of type constructors and other inversion
lemmas are not easy to establish from the declarative formulations of
judgmental equality.

Establishing such properties is however not the whole battle, earlier
formulations of this proof (cite Abel-Schrerer, also TYPES abstract)
relied on a second distinct logical relation with its own fundamental
lemma to prove that algorithmic equality is complete, i.e. it is
implied by judgmental equality. The two logical relations differ by
which equality relation they imply through their ``escape'' lemmas, in one
case judgmental in the other algorithmic.

Using two logical relations and fundamental lemmas might be the most
pragmatic presentation when doing things informally: most cases of the
proof will be omitted anyway, and it would otherwise be hard to keep
track of all the assumptions necessary if one tries to create an
abstraction to reuse. However when it comes to formalized proofs the
incentives and difficulties are reversed: you do have to provide
complete proofs and the typechecker helps you keep track of whether
you are missing something. So after formalizing the fundamental lemma
for the first logical relation we abstracted over judgmental equality
in the definition of the relation and collected the properties needed
for the proof of the fundamental lemma, while also keeping an eye on
what would be easily provable for algorithmic equality.  Doing so we
obtained a notion we called ``generic equality'' which we use to
parametrize the reducibility relation and its fundamental
lemma\footnote{Here I'd like to insert a special acknowlegment to
  Joakim, who has performed the bulk of the formalization and derived
  this abstraction}. We cna then instantiate this parametrized result
twice, for the two equalities of interest, to complete the overall
proof of decidability of conversion.

%% TODO Maybe say what things about judg. equality we need to prove that
%%   algorithmic equality is a generic equality. To make it clear why we need two stages like this.

While the article only handles a fairly minimal type theory, we
believe this formalization can form a good basis for applying the
proof technique to more complex theories. For example the kripke
structure could be extended to contain more than just renamings, but
also maps from the indexing category for type theories whose
denotational model is based on presheaf models. An example of a
similar setup can be seen in the proof of canonicity for Cubical Type
Theory (cite simon).

This formalization can also be seen as a step towards bootstrapping
Agda, as it could reasonably be extended into a double checker for the
core calculus.


%% \begin{itemize}
%%  \item previous part proposes extensions of type theory.
%%  \item such extensions better be consistent and usefully increase expressivity, but also be implementable.
%%  \item Proof assistants like Coq, Agda and Idris insist on a decidable judgmental equality and normal forms for terms.
%%  \item intensional type theory with decidable equality/normalization has plenty of nice properties simplifying an implementation:
%%    \begin{itemize}
%%    \item liberally run programs in types, useful for small scale reflection, program inference
%%    \item not only no need to store equality derivations, but can erase/omit types in eliminations (and introductions with bidirectionality) in terms
%%      \begin{itemize}
%%      \item normalization used to show equivalence between type-annotated and type-free terms (cite internship with coquand and peter).
%%          \begin{itemize}
%%          \item eliminations only left applied to neutrals, whose types can be inferred.
%%          \end{itemize}
%%      \end{itemize}

%%    \item extensional type theory implementations like NuPRL and Andromeda
%%    offer you program construction tactics so that the user can have finer control over the construction of typing derivations.
%%    This mitigates some drawbacks but not all (annotations left?).
%%    \end{itemize}

%%  \item When extending ITT one would like to prove such nice properties are
%%  preserved, and ultimately that a reasonable typechecking algorithm
%%  can be deployed. A big part is proving that there's an algorithm for
%%  deciding conversion.
%%  It's not an easy task, a major complication is that with universes types can have arbitrary reduction
%%  and given that we support eta rules equality of terms depends on their type.
%%  %%See (1) a bit later.

%%  \item One option is to prove a normalization theorem by for example
%%    Normalization By Evaluation as we did above, then the algorithm is ``fully normalize then compare''.
%%    \begin{itemize}
%%    \item That's not what happens in Agda or minitt where the algorithm is ``whnf, then compare heads, then recurse in subterms''
%%    \item If normalization happens lazily then the two algorithms can actually have very similar executions, but that's a quite brittle correspondence.

%%    \item However a practical elaborator/typechecker might have to
%%    interleave things with unification, and then the full normalization
%%    would get in the way of efficiency: great care is taken to avoid
%%    unnecessary reductions, also you would still need to propagate
%%    types and everything when comparing the normal forms so might as
%%    well interleave reduction in?
%%    Also it's relatively easy to introduce metas to this: when
%%    comparing heads if one side is a meta applied to some arguments,
%%    you can try to solve it.
%%    \end{itemize}

%%  \item So here in this paper we take the option of formalizing the
%%  correctness of a conversion checking algorithm based on typed whnf and
%%  type-directed comparison, the latter needed for eta rules on singleton types.

%%  \item we establish algorithmic equality/conversion as its own thing, a
%%  relation, for which we show decidability, and equivalence to
%%  judgmental equality

%%  \item regarding the relationship with normalization, from a proof that a
%%  term is algorithmically equal to itself we can extract by induction a
%%  normal form for the term and a proof that it's j.equal to the
%%  original.

%%  \item proof goes by kripke logical relation with contexts as worlds and renamings as morphisms between worlds.

%%    by analogy to gluing constructions, could reasonably expected to
%%    be extended to new theories with presheaf models by taking richer
%%    morphisms than just renamings.
%%    Might need to deploy kripke trick more often, because fewer
%%    predicates might be naturally monotonic, also might need to care
%%    about naturality.
%%    TODO: maybe check how hard it would be to integrate simon's proof.

%%  \item the kripke part handles open terms.

%%  \item the logical relation is based on reduction, we look at the whnf of
%%  things, for types it's inductive and just recurses in the subterms,
%%  for terms it's recursive on the proof of type reducibility, and checks that
%%  the observations that apply to the term whnf, according to the whnf of
%%  the type, should also be reducible.
%%    \begin{itemize}
%%    \item (1) once the fundamental lemma is proven, this helps clarifying
%%      the meaning of the typing and equality
%%      judgments, intuitive things like injectivity of type constructors,
%%      and other inversions are not easy to establish given
%%      the declarative formulation of judgmental equality
%%      and computation in types. (check the consequences file)

%%    \item the fundamental theorem is proven as usual by quantifying over a
%%    reducible substitution (however we package this quantification in
%%    the $||-^v$ relation), and by showing that the identity substitution
%%    is reducible.
%%    \end{itemize}

%%  \item Earlier proofs relied on two distinct logical relations with their
%%  own fundamental lemmas, one to establish the aforementioned inversion
%%  properties, another to prove the algorithm is complete.
%%  The two relations differ by which equality relation is used to
%%  compare terms, judgmental or algorithmic equality/conversion.

%%  Two log. rels and lemmas might be the most pragmatic presentation in
%%  a paper: you are not going to write out the proof for all the cases
%%  again, and it would otherwise be hard to keep track of all the
%%  assumptions necessary if one tries to create an abstraction to reuse.

%%  When it comes to formalized proofs the incentives and difficulties
%%  are reversed: you do have to provide complete proofs and the
%%  typechecker keeps track of whether you are missing something. So
%%  after formalizing the fundamental lemma for the first log. rel. we
%%  abstracted over judgmental equality in the definition of the relation
%%  and the properties needed for the fundamental lemma to go through
%%  while also keeping an eye on what would be easily provable for
%%  algorithmic equality.  We obtained a notion we called ``generic
%%  equality'' which we use to parametrize the fundamental
%%  lemma\footnote{Here I'd like to insert a special acknowlegment to
%%    Joakim, who has performed the bulk of the formalization and derived
%%    this abstraction}, which we then instantiate twice for the two
%%  equalities of interest.
%%  Maybe say what things about judg. equality we need to prove that
%%    algorithmic equality is a generic equality. To make it clear why we need two stages like this.

%%  \item We believe this formalization can form a good basis for applying
%%  the proof technique to more complex theories, though we might need
%%  some more abstraction passes to avoid too much copy-pasting programming.

%%  \item
%%    by analogy to gluing constructions, could reasonably expected to
%%    be extended to new theories with presheaf models by taking richer
%%    morphisms than just renamings.
%%    Might need to deploy kripke trick more often, because fewer
%%    predicates might be naturally monotonic, also might need to care
%%    about naturality.
%%    TODO: maybe check how hard it would be to integrate simon's proof.

%%  \item Easy steps: more automated substition development, extension to
%%  more of the standard type formers, recursive types. generalize to universe hierarchy.

%%  \item Hard steps: how do you programmatically reuse proofs about e.g. Pi
%%  types when typing judgments themselves might have to change? Maybe
%%  just better to have easily modifiable code than trying to predict a
%%  good abstraction (e.g. plenty of nothing, other modality-annotated pi types, ..., how to fit them in?).

%%  \item Also doubles as a first step in bootstrapping Agda: could be
%%  reasonably turned into a double checker for the core calculus.
%% \end{itemize}

\subsection{Statement of Contribution}
\paragraph{First part}
Contributed the proof without higher inductive types and participated
in writing the article.

\paragraph{Second Part}
Participated in the design and contributed most of the formalization
of the strong normalization proof and collaborated writing the
article.

Developed GCTT and its prototype implementation in collaboration with
Hans Bugge Grathwohl, contributed the proof that bisimulation is
equivalent to equality, participated in writing the article.

Contributed the application of parametricity to sizes and the proofs
and writeup of the construction of (co)inductive types by induction on
sizes and participated in the writing of other sections. Contributed
to the formulation of the model as modified cubical sets and the use
of the cohesive structure of the category. Contributed the use of a
modified Glue construction to internalize naturality with regard to
functions, and the prototype implementation as a fork of Agda.

Participated in the design of the type system and decidability of
conversion proof in Normalization by Evaluation for Sized Types.
\paragraph{Third Part}
Contributed to the formalization, in particular in setting up the
logical relation and the reasoning with substitutions. Collaborated
to the writing of the article.
\end{document}
